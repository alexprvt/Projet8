{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>PROJET 8: \"Déployez un modèle dans le cloud\"\n",
    "\n",
    "**Configuration:**\n",
    "- Ce notebook est exécuté sur une instance AWS EC2, de type t2.medium (2 processeurs) RAM de 4Go, sur un système d'explotation Ubuntu 18.04. Afin de garantir la sécurité d'utilisation, la connexion à cette instance est effectué via un tunnel SSH permettant d'accéder à un terminal de l'instance. \n",
    "- Le script contenu dans ce notebook ne peuvent fonctionner qu'après avoir téléchargé et installé Anaconda 3 (version 2019/03), Java 8, Scala 12, ainsi que Spark version 2.4.7 avec hadoop 2.7.\n",
    "- Il est nécessaire d'intaller les packages suivants: findspark, boto3, opencv-python, numpy\n",
    "- Cette instance est capable d'accéder au compartiments S3 de mon compte AWS, grâce au rôle IAM qui lui a été attribué. Il n'est donc pas nécessaire de configurer les credentials AWS au début du script. \n",
    "- L'accès à ce notebook jupyter se fait via serveur web, comme indiqué dans ce tutoriel: https://openclassrooms.com/fr/courses/4452741-decouvrez-les-librairies-python-pour-la-data-science/5559821-lancez-une-session-de-notebook-jupyter-sur-aws\n",
    "\n",
    "**Contexte**\n",
    "- Ce notebook permet d'évaluer la faisabilité d'établissement d'une chaîne de prétraitement de données visuelles.\n",
    "- Le données (images de fruits contenues dans un dossier par catégorie) sont stockées sur le cloud en utilisant le service s3.\n",
    "- Nous utiliserons Spark afin de parralléliser les opérations de calcul sur tous les processeurs disponibles.\n",
    "- L'objectif de ce script est d'importer les images, extraires leurs catégories, extraire les points d'intérêt de chaque image (avec descripteurs ORB), et réduire la dimension des descripteurs des points d'intérêt de chaque image avec une ACP. Les résultats de ces transformations seront enfin transférées et stockées dans un dossier 'results' dans le bucket S3 contenant les données initiales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Sommaire:\n",
    "### [I. Chargement des images dans un dataframe Spark](#section)\n",
    "### [II. Détection et extraction de features avec descripteurs ORB](#section2)\n",
    "### [III. Réduction dimensionnelle des descripteurs (ACP)](#section3)\n",
    "### [VI. Enregistrement des résultats sur le répertoire S3](#section4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Nous permettra de charger les dépendances nécessaires à certaines focntionnalités de Spark (hadoop-aws et java-aws)\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.amazonaws:aws-java-sdk-pom:1.11.538,org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour démarrer le notebook, nous utiliserons la méthode init de Findspark. Cela permet d'ajouter un fichier de démarrage au profil IPython actuel afin que les variables d'environnement soient correctement définies et que pyspark soit importé au démarrage d'IPython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons importer les différents modules et librairies que nous utiliserons pour effectuer les différentes transformations sur les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3  # facilite l'intéraction des instances EC2 avec S3 \n",
    "import numpy as np  # permet de redimensionner boto3\n",
    "\n",
    "# Permet de créer une session Spark qui joue de rôle de pilote (Driver)\n",
    "# de la façon dont Spark exécute les fonctions dans l’ensemble du cluster\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Fonctions permettant de maipuler les dataframes Spark (transformations)\n",
    "from pyspark.sql.functions import lit  # permet de créer une colonne de type littéral (str)\n",
    "from pyspark.sql.functions import udf  # permet d'appliquer une fonction donnée sur un df (user defined function)\n",
    "from pyspark.sql.functions import explode  # retourne une ligne pour chaque élément d'une colonne donnée\n",
    "from pyspark.sql.types import *  # permettra de manipuler tout tyle de données dans les udf\n",
    "from functools import reduce  # permet de fusionner des dataframes spark\n",
    "\n",
    "# Fonctions de machine learning pour dataframes spark\n",
    "from pyspark.ml.image import ImageSchema  # permet de lire et stocker les caractéristiques d'une image\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT  #  permet de transformer un ojet de type liste en un vecteur spark\n",
    "from pyspark.ml.feature import PCA  # permet d'appliquer une analyse en composantes princiaples sur des vecteurs\n",
    "import cv2 as cv  # bibliothèque OpenCV contenant les focntions de descriptions ORB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>I. Chargement des images dans un dataframe Spark<u><a name=\"section1\"></a><u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons d'abord initialiser une session Spark qui fera office de noeud maître, et permettra la parallélisation et la répartition entre les différents processeur disponibles. (Système Driver - Workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pilote (Driver) qui sera chargé de commander les processeurs en parallèle,\n",
    "# pour effectuer les prochaines transformations et opérations spark\n",
    "spark = (SparkSession.builder.appName(\"Prevot-P8\")\\\n",
    "         .config('spark.hadoop.fs.s3a.impl',\n",
    "                 'org.apache.hadoop.fs.s3a.S3AFileSystem')\\\n",
    "         .getOrCreate())\n",
    "\n",
    "# SparkContext est le point d’accès à toutes les fonctionnalités de Spark,\n",
    "# Il est contenu dans la Spark Session\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.us-east-2.amazonaws.com\")  # configure accès S3\n",
    "sc.setSystemProperty('com.amazonaws.services.s3.enableV4', 'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons désormais affecter une première tâche: charger les images dans un dataframe, et obtenir les étiquettes de chaque image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://p8-prevot-2/data2/Test/apple_pink_lady_1/\n",
      "s3a://p8-prevot-2/data2/Test/carrot_1/\n",
      "s3a://p8-prevot-2/data2/Test/cucumber_3/\n",
      "s3a://p8-prevot-2/data2/Test/pear_1/\n",
      "s3a://p8-prevot-2/data2/Test/zucchini_1/\n"
     ]
    }
   ],
   "source": [
    "bucket='p8-prevot-2'  # nom du bucket s3 contenant le dossier des images\n",
    "prefix='data2/Test/'  # chemin d'accès vers les dossiers d'images\n",
    "\n",
    "client = boto3.client('s3')  # objet de liaison vers S3 de mon compte AWS\n",
    "result = client.list_objects(Bucket=bucket, Prefix=prefix, Delimiter='/')  # permet de charger le contenu du bucket\n",
    "\n",
    "dataframes = []  # nous allons stocker une liste de dataframes (un df par dossier d'image, donc par type de fruit)\n",
    "\n",
    "for o in result.get('CommonPrefixes'):  \n",
    "    folder = o.get('Prefix')\n",
    "    sub_folder = folder.split('/')[2]\n",
    "    data_path = 's3a://{}/{}'.format(bucket, folder)\n",
    "    print(data_path) # affichage du sous-dossier\n",
    "    # Création d'un dataframe spark contenant les images et leurs caractéristiques\n",
    "    images_df = ImageSchema.readImages(data_path, recursive=True).withColumn(\"label\", lit(sub_folder))\n",
    "    dataframes.append(images_df)  # ajout à la liste des dataframes\n",
    "\n",
    "df = reduce(lambda first, second: first.union(second), dataframes)  # fusion des dfs (ayant tous le même schema)\n",
    "df = df.repartition(200) # répartiton en plusieurs partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque ligne du dataframe correspond à une image. Pour chaque image nous disposons de:\n",
    "- Première colonne (image): origin = chemin d'accès au fichier sur s3\n",
    "- Première colonne (image): height = hauteur (pixels)\n",
    "- Première colonne (image): width = largeur (pixels)\n",
    "- Première colonne (image): nChannels = 3 (RVB)\n",
    "- Première colonne (image): mode = format d'encodage de l'image\n",
    "- Première colonne (image): data = tableau contenant l'image encodée en binary\n",
    "- Deuxième colonne (label): Etiquette de l'image (fruit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = false)\n",
      " |    |-- width: integer (nullable = false)\n",
      " |    |-- nChannels: integer (nullable = false)\n",
      " |    |-- mode: integer (nullable = false)\n",
      " |    |-- data: binary (nullable = false)\n",
      " |-- label: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|               image|            label|\n",
      "+--------------------+-----------------+\n",
      "|[s3a://p8-prevot-...|       zucchini_1|\n",
      "|[s3a://p8-prevot-...|       zucchini_1|\n",
      "|[s3a://p8-prevot-...|       zucchini_1|\n",
      "|[s3a://p8-prevot-...|       zucchini_1|\n",
      "|[s3a://p8-prevot-...|       zucchini_1|\n",
      "|[s3a://p8-prevot-...|       cucumber_3|\n",
      "|[s3a://p8-prevot-...|       cucumber_3|\n",
      "|[s3a://p8-prevot-...|       cucumber_3|\n",
      "|[s3a://p8-prevot-...|       cucumber_3|\n",
      "|[s3a://p8-prevot-...|       cucumber_3|\n",
      "|[s3a://p8-prevot-...|           pear_1|\n",
      "|[s3a://p8-prevot-...|           pear_1|\n",
      "|[s3a://p8-prevot-...|           pear_1|\n",
      "|[s3a://p8-prevot-...|           pear_1|\n",
      "|[s3a://p8-prevot-...|           pear_1|\n",
      "|[s3a://p8-prevot-...|apple_pink_lady_1|\n",
      "|[s3a://p8-prevot-...|apple_pink_lady_1|\n",
      "|[s3a://p8-prevot-...|apple_pink_lady_1|\n",
      "|[s3a://p8-prevot-...|apple_pink_lady_1|\n",
      "|[s3a://p8-prevot-...|apple_pink_lady_1|\n",
      "+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>II. Détection et extraction de features avec descripteurs ORB<u><a name=\"section2\"></a><u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORB_UDF OK\n"
     ]
    }
   ],
   "source": [
    "def orb_descriptors(img):\n",
    "    ''' Fonction prenant en entrée une image du dataframe \n",
    "    et qui renvoie la liste des descripteurs ORB des points d'intérêts détectés sur l'image'''\n",
    "    \n",
    "    height = img[1]\n",
    "    width = img[2]\n",
    "    nchannels = img[3]\n",
    "    data = img[5]\n",
    "    \n",
    "    img_array = np.array(data).reshape(height, width, nchannels)  # conversion et redimensionnement de l'image\n",
    "    \n",
    "    orb = cv.ORB_create(nfeatures=30)  # création d'un détecteur orb (30 points d'intérêts maximum)\n",
    "    kp, des = orb.detectAndCompute(img_array,None)  # détection et description des pts d'intérêts\n",
    "    \n",
    "    des = des.tolist()  # on a un tableau contenant des descripteurs de 32 composantes chacuns\n",
    "    \n",
    "    return des\n",
    "\n",
    "# user define function permettant la création de la colonne descripteurs suiveant la fonction \"orb_descriptors\"\n",
    "orb_UDF = udf(lambda img: orb_descriptors(img), ArrayType(ArrayType(IntegerType())))\n",
    "df = df.withColumn(\"Descripteurs ORB\", orb_UDF(\"image\"))  # application de l'udf sur le dataframe spark\n",
    "print('ORB_UDF OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = false)\n",
      " |    |-- width: integer (nullable = false)\n",
      " |    |-- nChannels: integer (nullable = false)\n",
      " |    |-- mode: integer (nullable = false)\n",
      " |    |-- data: binary (nullable = false)\n",
      " |-- label: string (nullable = false)\n",
      " |-- Descripteurs ORB: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une ligne correspond a une image. Dans la colonne 'Descripteurs ORB', il y a un nombre non défini de descripteurs de chaque image (liste de liste de 32 composantes entières)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------+\n",
      "|               image|            label|    Descripteurs ORB|\n",
      "+--------------------+-----------------+--------------------+\n",
      "|[s3a://p8-prevot-...|       zucchini_1|[[222, 253, 148, ...|\n",
      "|[s3a://p8-prevot-...|       zucchini_1|[[134, 96, 198, 1...|\n",
      "|[s3a://p8-prevot-...|       zucchini_1|[[52, 201, 150, 1...|\n",
      "|[s3a://p8-prevot-...|       zucchini_1|[[82, 48, 118, 16...|\n",
      "|[s3a://p8-prevot-...|       zucchini_1|[[162, 172, 214, ...|\n",
      "|[s3a://p8-prevot-...|       cucumber_3|[[120, 160, 244, ...|\n",
      "|[s3a://p8-prevot-...|       cucumber_3|[[230, 141, 226, ...|\n",
      "|[s3a://p8-prevot-...|       cucumber_3|[[134, 173, 230, ...|\n",
      "|[s3a://p8-prevot-...|       cucumber_3|[[54, 113, 118, 2...|\n",
      "|[s3a://p8-prevot-...|       cucumber_3|[[246, 44, 246, 1...|\n",
      "|[s3a://p8-prevot-...|           pear_1|[[188, 249, 2, 23...|\n",
      "|[s3a://p8-prevot-...|           pear_1|[[146, 229, 118, ...|\n",
      "|[s3a://p8-prevot-...|           pear_1|[[34, 1, 98, 197,...|\n",
      "|[s3a://p8-prevot-...|           pear_1|[[218, 64, 118, 1...|\n",
      "|[s3a://p8-prevot-...|           pear_1|[[80, 228, 34, 19...|\n",
      "|[s3a://p8-prevot-...|apple_pink_lady_1|[[70, 173, 246, 2...|\n",
      "|[s3a://p8-prevot-...|apple_pink_lady_1|[[44, 169, 244, 1...|\n",
      "|[s3a://p8-prevot-...|apple_pink_lady_1|[[42, 48, 226, 14...|\n",
      "|[s3a://p8-prevot-...|apple_pink_lady_1|[[22, 229, 70, 17...|\n",
      "|[s3a://p8-prevot-...|apple_pink_lady_1|[[54, 255, 230, 1...|\n",
      "+--------------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons 'exploser' le dataframe afin d'avoir une ligne par descripteur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explode OK\n"
     ]
    }
   ],
   "source": [
    "df = df.select(df['image'], df['label'], explode(df['Descripteurs ORB']))\n",
    "df = df.withColumnRenamed(\"col\",\"Descripteur\")\n",
    "print('Explode OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = false)\n",
      " |    |-- width: integer (nullable = false)\n",
      " |    |-- nChannels: integer (nullable = false)\n",
      " |    |-- mode: integer (nullable = false)\n",
      " |    |-- data: binary (nullable = false)\n",
      " |-- label: string (nullable = false)\n",
      " |-- Descripteur: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|               image|     label|         Descripteur|\n",
      "+--------------------+----------+--------------------+\n",
      "|[s3a://p8-prevot-...|zucchini_1|[222, 253, 148, 1...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[252, 68, 254, 22...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[188, 68, 111, 10...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[184, 69, 110, 23...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[130, 12, 134, 15...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[236, 252, 148, 2...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[128, 140, 6, 159...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[252, 68, 254, 22...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[213, 69, 111, 22...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[232, 189, 212, 2...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[153, 69, 111, 22...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[128, 141, 6, 159...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[252, 68, 238, 22...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[194, 188, 212, 2...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[184, 69, 110, 23...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[130, 140, 134, 1...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[194, 140, 134, 1...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[194, 172, 214, 1...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[220, 65, 254, 23...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[252, 64, 238, 22...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour réaliser l'ACP, il sera nécessaire de transformer le format des descripteurs (liste entiière) en vecteur spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization OK\n"
     ]
    }
   ],
   "source": [
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "df = df.withColumn(\"Descripteur-vect\", list_to_vector_udf(df[\"Descripteur\"]))\n",
    "df = df.drop('Descripteur')\n",
    "print('Vectorization OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = false)\n",
      " |    |-- width: integer (nullable = false)\n",
      " |    |-- nChannels: integer (nullable = false)\n",
      " |    |-- mode: integer (nullable = false)\n",
      " |    |-- data: binary (nullable = false)\n",
      " |-- label: string (nullable = false)\n",
      " |-- Descripteur-vect: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|               image|     label|    Descripteur-vect|\n",
      "+--------------------+----------+--------------------+\n",
      "|[s3a://p8-prevot-...|zucchini_1|[222.0,253.0,148....|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[252.0,68.0,254.0...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[188.0,68.0,111.0...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[184.0,69.0,110.0...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[130.0,12.0,134.0...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[236.0,252.0,148....|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[128.0,140.0,6.0,...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[252.0,68.0,254.0...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[213.0,69.0,111.0...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[232.0,189.0,212....|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[153.0,69.0,111.0...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[128.0,141.0,6.0,...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[252.0,68.0,238.0...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[194.0,188.0,212....|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[184.0,69.0,110.0...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[130.0,140.0,134....|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[194.0,140.0,134....|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[194.0,172.0,214....|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[220.0,65.0,254.0...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[252.0,64.0,238.0...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>III. Réduction dimensionnelle des descripteurs (ACP)<u><a name=\"section3\"></a><u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est alors possible de réaliser une analyse en composante principale sur 20 composantes (par exemple) afin de réduire la taille des descripteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA OK\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(k=20, inputCol=\"Descripteur-vect\", outputCol=\"Descripteur-ACP\")\n",
    "pca = pca.fit(df)\n",
    "df = pca.transform(df)\n",
    "df = df.drop('Descripteur-vect')\n",
    "print('PCA OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|               image|     label|     Descripteur-ACP|\n",
      "+--------------------+----------+--------------------+\n",
      "|[s3a://p8-prevot-...|zucchini_1|[367.946231808941...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[109.782596723375...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[54.5693452270751...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[100.421663261629...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[493.223592675236...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[488.019373538996...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[535.950474403000...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[103.977681023866...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[118.961349416184...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[425.006237382828...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[124.416240677825...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[487.704364341379...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[84.2301011644544...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[493.990015016403...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[107.326118706153...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[535.953623909773...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[467.709827738212...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[467.744437127256...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[101.720929218525...|\n",
      "|[s3a://p8-prevot-...|zucchini_1|[83.5573513532216...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>VI. Enregistrement des résultats sur le répertoire S3<u><a name=\"section4\"></a><u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons réalisé les premières briques de la châine de traitement. Il faut désormais stocker sur S3 le dataframe afin de pouvoir y accéder de nouveau pour la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write OK\n"
     ]
    }
   ],
   "source": [
    "df.write.parquet(path=\"s3a://p8-prevot-2/results/\", mode=\"overwrite\")\n",
    "print('Write OK')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
